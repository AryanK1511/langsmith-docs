---
sidebar_label: Quick Start
sidebar_position: 1
table_of_contents: true
---

import {
  CodeTabs,
  python,
  typescript,
  ShellBlock,
} from "@site/src/components/InstructionsWithCode";
import { RegionalUrl } from "@site/src/components/RegionalUrls";

# Evaluation quick start

This quick start will get you up and running with our evaluation SDK and Experiments UI.

## 1. Install Dependencies

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      language: "bash",
      content: `pip install -U langsmith openai`,
    },
    {
      value: "typescript",
      label: "TypeScript",
      language: "bash",
      content: `yarn add langsmith openai`,
    },
  ]}
  groupId="client-language"
/>

## 2. Create an API key

To create an API key head to the <RegionalUrl text='Settings page' suffix='/settings' />. Then click **Create API Key.**

## 3. Set up your environment

<CodeTabs
  tabs={[
    ShellBlock(`export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
# The example uses OpenAI, but it's not necessary in general
export OPENAI_API_KEY=<your-openai-api-key>`),
  ]}
  groupId="client-language"
/>

## 4. Create a dataset

<CodeTabs
  tabs={[
    {
      value: "python",
      label: "Python",
      content: `from langsmith import evaluate, Client

client = Client()

# Create inputs and reference outputs
example_inputs = [
(
    "Which country is Mount Kilimanjaro located in?",
    "Mount Kilimanjaro is located in Tanzania.",
),
(
    "What is Earth's lowest point?",
    "Earth's lowest point is The Dead Sea.",
)
]

# Programmatically create a dataset in LangSmith
dataset = client.create_dataset(
    dataset_name="Sample dataset",
    description="A sample dataset in LangSmith."
)

inputs = [{"question": input_prompt} for input_prompt, _ in example_inputs]
outputs = [{"answer": output_answer} for _, output_answer in example_inputs]

# Add examples to the dataset
client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)`,
    },
    {
      value: "typescript",
      label: "TypeScript",
      content: `
import { Client } from "langsmith";
import { evaluate } from "langsmith/evaluation";

const client = new Client();

// Create inputs and reference outputs
const exampleInputs: [string, string][] = [
[
"Which country is Mount Kilimanjaro located in?",
    "Mount Kilimanjaro is located in Tanzania.",
],
[
 "What is Earth's lowest point?",
    "Earth's lowest point is The Dead Sea.",
],
];

// Programmatically create a dataset in LangSmith
const dataset = await client.createDataset("Sample dataset", {
description: "A sample dataset in LangSmith.",
});

const inputs = exampleInputs.map(([inputPrompt]) => ({
question: inputPrompt,
}));
const outputs = exampleInputs.map(([, outputAnswer]) => ({
answer: outputAnswer,
}));

// Add examples to the dataset
await client.createExamples({
inputs,
outputs,
datasetId: dataset.id,
});
`,
},
]}
groupId="client-language"
/>

## 5. Define your evaluator

<CodeTabs
  tabs={[
    python({caption: "Requires `langsmith>=0.1.145`"})`from pydantic import BaseModel, Field
from openai import OpenAI
import json

openai_client = OpenAI()

# Define accuracy evaluation prompt       
GRADE_ACCURACY_PROMPT = """Evaluate Student Answer against Ground Truth for conceptual similarity and score 0 or 1:

- Score 0: No conceptual match and similarity
- Score 1: Most or full conceptual match and similarity
- Key criteria: Concept should match, not exact wording.

Ground Truth answer: {reference}
Student's Answer: {prediction}"""

class Response(BaseModel):
        score: int = Field(description="Score that indicates how accurate the response is relative to the reference answer")

# Run the application logic you want to evaluate
def new_pipeline_answer(inputs):
    response = openai_client.chat.completions.create(
      model="gpt-4o-mini",
      messages=[
              {
                "role": "system", 
                "content": "Answer the following question accurately and concisely"
              },
              {
                "role": "user", 
                "content": inputs["question"]
                },
            ]
        )
    
        return {"response": response.choices[0].message.content.strip()}

# Define LLM judge evaluator that grades the accuracy of the response relative to the reference output
def answer_accuracy_evaluator(outputs, reference_outputs) -> dict:
  grade_answer_accuracy_prompt_formatted = GRADE_ACCURACY_PROMPT.format(
      prediction=outputs["response"],
      reference=reference_outputs["answer"]
    )
  response = openai_client.beta.chat.completions.parse(
    model="gpt-4o-mini",
    messages=[
      {"role": "system", "content": grade_answer_accuracy_prompt_formatted},
    ],
    response_format=Response
  )
  return {
    "key": "accuracy",
    "score": json.loads(response.choices[0].message.content)["score"]
}
`,
typescript({caption: "Requires `langsmith>=0.2.9`"})`
import OpenAI from "openai";
import { z } from "zod";
import { zodResponseFormat } from "openai/helpers/zod";
import { Run, Example } from "langsmith";

const openai = new OpenAI();

// Define accuracy evaluation prompt
const GRADE_ACCURACY_PROMPT = \`Evaluate Student Answer against Ground Truth for conceptual similarity and score 0 or 1:

- Score 0: No conceptual match and similarity
- Score 1: Most or full conceptual match and similarity
- Key criteria: Concept should match, not exact wording.

Ground Truth answer: {reference}
Student's Answer: {prediction}\`

const ResponseSchema = z.object({
score: z
.number()
.describe(
"Score that indicates how accurate the response is relative to the reference answer"
),
});

// Run the application logic you want to evaluate
async function newPipelineAnswer(inputs: {
input: string;
}): Promise<{ response: string }> {
const response = await openai.chat.completions.create({
model: "gpt-4o-mini",
messages: [
{
role: "system",
content:
"Answer the following question accurately and concisely.",
},
{ role: "user", content: inputs.input },
],
});

return { response: response.choices[0].message.content?.trim() || "" };
}

// Define LLM judge evaluator that grades the accuracy of the response relative to the reference output
async function answerAccuracyEvaluator(
run: Run,
example?: Example
): Promise<{
key: string;
score: number;
}> {
const gradeAnswerAccuracyPromptFormatted = GRADE_ACCURACY_PROMPT.replace(
"{prediction}",
run.outputs?.response || ""
).replace("{reference}", example?.outputs?.answer || "");

const response = await openai.chat.completions.create({
model: "gpt-4o-mini",
messages: [{ role: "system", content: gradeAnswerAccuracyPromptFormatted }],
response_format: zodResponseFormat(ResponseSchema, "response"),
});

const parsedResponse = ResponseSchema.parse(
JSON.parse(response.choices[0].message.content || "")
);

return {
key: "accuracy",
score: parsedResponse.score,
};
}

`,
]}
groupId="client-language"
/>

## 6. Run and view results

<CodeTabs
  tabs={[
    python`
# After running the evaluation, a link will be provided to view the results in langsmith
experiment_results = evaluate(
      new_pipeline_answer,
      data = "Sample dataset",
      evaluators = [
          answer_accuracy_evaluator,
          # can add multiple evaluators here
      ],
      experiment_prefix = "first-eval-in-langsmith",
)

`,
    typescript`
// After running the evaluation, a link will be provided to view the results in langsmith
evaluate(
(exampleInput) => {
return newPipelineAnswer({ input: exampleInput.question });
},
{
data: "Sample dataset",
evaluators: [
answerAccuracyEvaluator,
// can add multiple evaluators here
],
experimentPrefix: "first-eval-in-langsmith",
}
);
`,
]}
groupId="client-language"
/>

Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.

![](./how_to_guides/evaluation/static/view_experiment.gif)

## Next steps

For conceptual explanations see the [Conceptual guide](./evaluation/concepts).
See the [How-to guides](./evaluation/how_to_guides) for answers to “How do I….?” format questions.
For end-to-end walkthroughs see [Tutorials](./evaluation/tutorials).
For comprehensive descriptions of every class and function see the [API reference](https://langsmith-sdk.readthedocs.io/en/latest/evaluation.html).
