# Evaluation Use-Case Guide

## Introduction

In this guide, we will provide an overview of popular evaluation use-cases.

:::tip Recommended Reading
See our LangSmith evaluation video series for a detailed overview of evaluation concepts:

- [Introductory video](https://youtu.be/vygFgCNR7WA?feature=shared) for an overview of general concepts.

:::

![use-case-overview.png](../static/use-case-overview.png)

## RAG

### Overview

RAG (retrieval augmented generation) retrieves documents that are related to a user input index and passes them to an LLM.

:::tip Recommended Reading
See our detailed video series and repo on RAG:

- [`RAG From Scratch` series](https://github.com/langchain-ai/rag-from-scratch) for an overview of general concepts.

:::

### Dataset

A central consideration for RAG evaluation is whether you have a dataset with `reference` answers or not. If so, `developer curated examples` will typically include pairs of questions and ground truth reference answers (see a dataset example [here](https://smith.langchain.com/public/730d833b-74da-43e2-a614-4e2ca2502606/d)). However, even without `reference` answers, several different evaluations are possible using any of the `Reference-free` RAG evaluation prompts (see examples below).

### Evaluator

`LLM-as-judge` is typically used for all RAG evaluations.

![rag-types.png](../static/rag-types.png)

`Ground truth reference` answers are commonly used to evaluate the RAG chain's generated answer:

- RAG answer correctness evaluation video: https://youtu.be/lTfhw_9cJqc?feature=shared

`Reference-free` prompts can also be used for various self-consistency checks (orange, green, and red in the above figure):

- RAG answer hallucination video: https://youtu.be/IlNglM9bKLw?feature=shared
- RAG document relevance video: https://youtu.be/Fr_7HtHjcf0?feature=shared

### Applying RAG evaluation

`Offline` evaluation is used for any prompts that rely on a `reference`. Most commonly, this is used with RAG answer correctness evaluation where the reference is a ground truth (correct) answer.

`Online` evaluation can be used for any `reference-free` prompts:

- RAG with online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared

`Pairwise` evaluation is an effective way to compare answers produced by different RAG chains. Usually this kind of evaluation is focused on user specified criteria (e.g., to answer format or style) rather correctness (which can be evaluated using self-consistency or a ground truth reference):

- RAG pairwise evaluation video: https://youtu.be/yskkOAfTwcQ?feature=shared

### RAG evaluation summary

| Use Case            | Detail                                            | Reference-free? | LLM-as-judge?                                                                         | Pairwise relevant |
| ------------------- | ------------------------------------------------- | --------------- | ------------------------------------------------------------------------------------- | ----------------- |
| Document relevance  | Are documents relevant to the question?           | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-document-relevance)   | No                |
| Answer faithfulness | Is the answer grounded in the documents?          | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination) | No                |
| Answer helpfulness  | Does the answer help address the question?        | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness)   | No                |
| Answer correctness  | Is the answer consistent with a reference answer? | No              | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference)  | No                |
| Chain comparison    | How do multiple answer versions compare?          | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-rag)  | Yes               |

## Summarization

### Overview

Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.

### Dataset

`Developer curated examples` of texts to summarize are commonly used for evaluation (see a dataset example [here](https://smith.langchain.com/public/659b07af-1cab-4e18-b21a-91a69a4c3990/d)). However, `user logs` from a production (summarization) app can be used for online evaluation with any of the `Reference-free` evaluation prompts below.

### Evaluator

`LLM-as-judge` is typically used for evaluation of summarization (as well as other types of writing) using `Reference-free` prompts that follow provided criteria to grade a summary. It is less common to provide a particular `Reference` summary, because summarization is a creative task and there are many possible correct answers.

### Applying summarization evaluation

`Online` or `Offline` evaluation are feasible because of the `Reference-free` prompt used.

`Pairwise` evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):

- Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared

### Summarization evaluation summary

| Use Case         | Detail                                                                     | Reference-free? | LLM-as-judge?                                                                                | Pairwise relevant |
| ---------------- | -------------------------------------------------------------------------- | --------------- | -------------------------------------------------------------------------------------------- | ----------------- |
| Factual accuracy | Is the summary accurate relative to the source documents?                  | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-accurancy-evaluator)     | Yes               |
| Faithfulness     | Is the summary grounded in the source documents (e.g., no hallucinations)? | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-hallucination-evaluator) | Yes               |
| Helpfulness      | Is summary helpful relative to user need?                                  | Yes             | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-helpfulness-evaluator)   | Yes               |

## Classification / Tagging

### Overview

Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below:

### Dataset

A central consideration for Classification / Tagging evaluation is whether you have a dataset with `reference` labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc).

### Evaluator

If ground truth reference labels are provided, then it's common to simply define a [custom heuristic evaluator](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators) to compare ground truth labels to the chain output. However, it is increacingly common given the emergence of LLMs simply use `LLM-as-judge` to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference).

### Applying classification evaluation

`Online` or `Offline` evaluation is feasible when using `LLM-as-judge` with the `Reference-free` prompt used. In particular, this is well suited to `Online` evaluation when a user wants to tag / classifity application input (e.g., for toxicity, etc).

- Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared

### Classification / tagging evaluation summary

| Use Case  | Detail                          | Reference-free?         | LLM-as-judge?                                                        | Pairwise relevant |
| --------- | ------------------------------- | ----------------------- | -------------------------------------------------------------------- | ----------------- |
| Criteria  | Tag if specific criteria is met | Yes                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/tagging) | No                |
| Accuracy  | Standard definition             | No (ground truth class) | No                                                                   | No                |
| Precision | Standard definition             | No (ground truth class) | No                                                                   | No                |
| Recall    | Standard definition             | No (ground truth class) | No                                                                   | No                |
