---
sidebar_label: Run pairwise evaluations
sidebar_position: 8
---

# Run Pairwise Evaluations

:::note
Currently, `evaluate_comparative` is only supported in the Python SDK.
:::

LangSmith supports evaluating existing experiments against each other. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think [Chatbot Arena](https://chat.lmsys.org/) - this is the same concept! To do this, use the `evaluate_comparative` function
with two existing experiments. If you haven't already created experiments to compare, check out our [Quick Start guide](https://docs.smith.langchain.com/#5-run-your-first-evaluation) or our [Evaluations How-To Guide](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application) to get started with evaluations.

## Inputs and Outputs

**Inputs:** A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two.

**Output:** Your evaluator should return a dictionary with two keys: `key`, which represents the feedback key that will be logged, and `scores`, which is a
mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent
"both equally bad" or both to 1 for "both equally good".

Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise` or `ranked`.

## Example

```python
from langsmith.evaluation import evaluate_comparative

def evaluate_pairwise(runs: list, example):
    scores = {}
    for i, run in enumerate(runs):
        scores[run.id] = i

    return {"key": "ranked_preference", "scores": scores}


evaluate_comparative(
    # Replace the following array with the names or IDs of your experiments
    ["test-ordinary-jug-88", "test-mundane-strait-65"],
    evaluators=[evaluate_pairwise],
)
```

This example simply assigns the better score to each of the runs in `test-mundane-strait-65`. However, you can create any type of evaluator you would like: for example, using an LLM to judge between the two outputs.

## In LangSmith UI

Navigate to the "Pairwise Experiments" tab from the dataset page:

![Pairwise Experiments Tab](./static/pairwise_from_dataset.png)

Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:

![Pairwise Comparison View](./static/pairwise_comparison_view.png)

You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:

![Pairwise Filtering](./static/filter_pairwise.png)
